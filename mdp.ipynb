{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mdp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbipin/aip/blob/master/mdp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr2dA9HmkePh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we plan to implement some of the algorithms related to MDPs and RL\n",
        "#MDP study\n",
        "#%matplotlib inline\n",
        "#import matplotlib\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#I am trying to avoid the numpy dependencies\n",
        "\n",
        "import random\n",
        "#\n",
        "#We plan to implement the gridworld class \n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8O0Do5k-8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let us have a gridworld\n",
        "#ref: Chapter 17, Artificial Intelligence a Modern Approach\n",
        "#ref: CS188 https://inst.eecs.berkeley.edu/~cs188/fa19/\n",
        "#ref: https://inst.eecs.berkeley.edu/~cs188/fa19/assets/slides/lec8.pdf\n",
        "\n",
        "#This class will create a 2D grid of row x colums \n",
        "#Some of the cells can be disabled by putting it into walls\n",
        "#cells are addressed just like 2d arrays (r,c)\n",
        "#There are possibly many terminal states\n",
        "#terminal states have only one action available: Exit \n",
        "#Transistion is as per the book 80% action and 20%sideways\n",
        "\n",
        "#Actions #just some alias\n",
        "Up    = 0\n",
        "Right = 1\n",
        "Down  = 2\n",
        "Left  = 3\n",
        "Exit  = 4\n",
        "\n",
        "class GridWorld :\n",
        "    #Default is as given in the AIMA book\n",
        "    def __init__(self, \n",
        "                 rows    =3, \n",
        "                 columns =4, \n",
        "                 walls   =[(1,1)], terminals= {(0,3):+1.0, (1,3):-1.0}, \n",
        "                 gamma   =1.0, \n",
        "                 living_reward=0,\n",
        "                 noise   =0.2\n",
        "                ) :\n",
        "        \"\"\"We dont expect these parameters to change during the agent run\"\"\"\n",
        "        self.rows      = rows\n",
        "        self.columns   = columns\n",
        "        self.N         = rows * columns #total cells\n",
        "        self.walls     = walls\n",
        "        self.terminals = terminals #dictionary of terminal celss and their rewards.\n",
        "        self.gamma     = gamma\n",
        "        self.living_reward = living_reward\n",
        "        self.all_actions   = [ Up, Down, Right, Left, Exit ]\n",
        "        self.all_states    = [(r,c) for r in range(rows) for c in range(columns) if (r,c) not in walls ]\n",
        "        self.noise         = noise\n",
        "        self.end_state     = (-1, -1) #a dummy state to reach after taking Exit\n",
        "        \n",
        "        #transitions from each state and the probabilities\n",
        "        self.noise                = noise\n",
        "        self.action_transitions   = { \n",
        "            Up:   ([Up,    Left, Right], [1-noise, noise/2, noise/2 ]),\n",
        "            Down: ([Down,  Left, Right], [1-noise, noise/2, noise/2 ]),\n",
        "            Left: ([Left,  Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
        "            Right:([Right, Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
        "            Exit :([Exit], [1.0])\n",
        "        }\n",
        "    \n",
        "    def actions(self, state) :\n",
        "        \"\"\"returns all valid actions from the current state\"\"\"\n",
        "        if state in self.terminals :\n",
        "            return [Exit]\n",
        "        return [ Up, Down, Right, Left ]\n",
        "    \n",
        "    def reward(self, state, action) :\n",
        "        if state in self.terminals :\n",
        "            return self.terminals[state] #dict has the terminal values +1 or -1\n",
        "        return self.living_reward        #usually a small -ve value\n",
        "    \n",
        "    def transitions(self, state, action) :\n",
        "        \"\"\"returna list of tuple(nextstate, action, probability)\"\"\"\n",
        "        actual_actions, probs = self.action_transitions[action]\n",
        "        return [ self._next_cell(state, a) for a in actual_actions ], actual_actions, probs\n",
        "    \n",
        "    def move(self, state, action) :\n",
        "        \"\"\"Take the action and return the tuple(new_state, reward, is_terminal)\"\"\"         \n",
        "        if action == Exit :\n",
        "            new_state   = self.end_state\n",
        "            reward      = self.reward(state, action) #terminals got the rewards as well\n",
        "            is_terminal = True\n",
        "            \n",
        "        else :\n",
        "            #we find the new_cell of the slippery action according to the prbabilities\n",
        "            #but this cell may be invalid. If invalid, we stay in the current state.\n",
        "\n",
        "            #zip is basically it's own inverse.\n",
        "            #ref: https://stackoverflow.com/questions/12974474/how-to-unzip-a-list-of-tuples-into-individual-lists/12974504\n",
        "            #transitions = self.transitions(state, action)\n",
        "            #cells, actions, p = list(zip(*transitions))\n",
        "                  \n",
        "            cells, actions, p = self.transitions(state, action)\n",
        "            #we choose one cell acccording to probabilities\n",
        "            new_state   = random.choices(cells, weights=p)[0] #only one; we take index 0                \n",
        "            reward      = self.reward(state, action) #\n",
        "            is_terminal = False\n",
        "            \n",
        "        return new_state, reward, is_terminal #keep the same for mat as OpenAI gym.\n",
        "    \n",
        "    def _next_cell(self, state, action) : \n",
        "        \"\"\"Blindly takes the action without checking anything and returns the position\"\"\"\n",
        "        r,c = state #row & column\n",
        "        if action == Up :\n",
        "            target = r-1, c  \n",
        "        if action == Down :\n",
        "            target = r+1, c\n",
        "        if action == Right :\n",
        "            target = r, c+1  \n",
        "        if action == Left :\n",
        "            target = r, c-1\n",
        "        \n",
        "        if self._valid_cell(target) :\n",
        "            return target\n",
        "        return state #stay put the target is invalid.\n",
        "    \n",
        "    def _valid_cell(self, cell) :\n",
        "        \"\"\"Returns true if the cell is a valid cell\"\"\"\n",
        "        r, c = cell #this may be an illegal node; we need to check\n",
        "        \n",
        "        #is it any of the walls?\n",
        "        if (r,c) in self.walls :\n",
        "            return False\n",
        "        \n",
        "        #is it outside the grid?\n",
        "        if r < 0 or r >= self.rows or c < 0 or c >= self.columns :\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    #pretty print the grid and agent if given.\n",
        "    def print(self, agent_state=None) :\n",
        "        for r in range(self.rows) :\n",
        "            for c in range(self.columns) :\n",
        "                cell = (r,c)\n",
        "                if cell in self.walls :\n",
        "                    print('# ', end='')\n",
        "                elif cell in self.terminals :\n",
        "                    if self.terminals[cell] > 0 :\n",
        "                        print('+', end=' ')\n",
        "                    else :\n",
        "                        print('-', end=' ')\n",
        "                elif cell == agent_state :\n",
        "                    print('@ ', end='')\n",
        "                else :\n",
        "                    print('. ', end='')\n",
        "            print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jw8OLkqtvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlt4WidlLos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_world = GridWorld(gamma=0.9, living_reward=-0.04)\n",
        "start = (2,0) #as in the book"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoZp-RtKlQEB",
        "colab_type": "code",
        "outputId": "3873c78a-9056-4680-b83b-d31d106f9ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# + and - are the terminal states. @ is our agent.\n",
        "grid_world.print(start)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". . . + \n",
            ". # . - \n",
            "@ . . . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARM2HAQElCM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############# I might change the API later ##########\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiIB6IdtlHJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy :\n",
        "    def __init__(self, grid_world=None) :\n",
        "        \"\"\"Holds one policy and returns actions according to it\"\"\"\n",
        "        self.grid_world = grid_world\n",
        "        self.policy     = { } #{ state: policy_action}\n",
        "        \n",
        "    def __getitem__(self, state) :\n",
        "        return self.policy[state]\n",
        "    \n",
        "    def __setitem__(self, state, action) :\n",
        "        self.policy[state] = action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8uac7g9lb46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Some test code. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh2ZfGg2logH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_policy(grid_world) :\n",
        "    \"\"\"returns a random action function\"\"\"\n",
        "    def f(state) :\n",
        "        return random.choice(grid_world.actions(state))\n",
        "    return f\n",
        "\n",
        "def fixed_policy(grid_world) :\n",
        "    p = Policy()\n",
        "    p.policy = {state: Up   for state in grid_world.all_states }\n",
        "    p.policy.update({state:Exit for state in grid_world.terminals})\n",
        "    #print(p.policy)\n",
        "    def f(state) :\n",
        "        return p[state]\n",
        "    return f\n",
        "\n",
        "def good_policy(grid_world) :\n",
        "    p = Policy()\n",
        "    p.policy = {\n",
        "        (0,0):Right, (0,1): Right, (0,2): Right, (0,3) : Exit,\n",
        "        (1,0):Up,    (1,1): Right, (1,2): Up,    (1,3) : Exit,\n",
        "        (2,0):Up,    (2,1): Left,  (2,2): Up,    (2,3) : Left,\n",
        "               }\n",
        "    p.policy.update({state:Exit for state in grid_world.terminals})\n",
        "    #print(p.policy)\n",
        "    def f(state) :\n",
        "        return p[state]\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQG9ksZrGX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(grid_world, state, policy) :\n",
        "    \"\"\"runs a full episode and return the total reward\"\"\"\n",
        "    rewards = []\n",
        "    gamma = grid_world.gamma\n",
        "    time_step = 0\n",
        "    while True :\n",
        "        action = policy(state)\n",
        "        #a.print()\n",
        "        #print(action)\n",
        "        state, r, exited = grid_world.move(state, action)\n",
        "        rewards.append(r * (gamma**time_step) ) #the further we go down, the less we value the reward\n",
        "        if exited :\n",
        "            break\n",
        "    \n",
        "        time_step += 1\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def expected_utility(grid_world, state, policy, N=100) :\n",
        "    \"\"\"run the policy till completion several times and return the expected utility\"\"\"\n",
        "    s = 0.0\n",
        "    for _ in range(N) :\n",
        "        #from the same start state we run till completion, N times\n",
        "        s += sum( run(grid_world, state, policy) )\n",
        "    return s/N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te1GnYPLlpNz",
        "colab_type": "code",
        "outputId": "6755d45b-ab5e-466a-e7b6-39b2ed3aa951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "#page  651; AIMA Book\n",
        "#The utilities of the states in the 4 × 3 world, calculated with γ = 1 and\n",
        "#R(s) = − 0.04 for nonterminal states\n",
        "\n",
        "grid_world = GridWorld(gamma=1.0, living_reward=-0.04)\n",
        "\n",
        "for cell in grid_world.all_states :\n",
        "    print (expected_utility(grid_world, cell, good_policy(grid_world), 10000))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8059440000001052\n",
            "0.8696080000000642\n",
            "0.9149199999999595\n",
            "1.0\n",
            "0.7589440000001115\n",
            "0.6405880000000638\n",
            "-1.0\n",
            "0.7086120000001105\n",
            "0.6574880000000892\n",
            "0.5939000000000813\n",
            "0.37200800000003836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKEfEoBcltXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}